{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "712ae772",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Final Project Presentation\n",
    "\n",
    "## Andrew Bahsoun\n",
    "\n",
    "## 11 December 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec4684a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd34b13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Project Details\n",
    "Image Classification Model to determine what step someone is on when washing their hands.\n",
    "\n",
    "<img src=\"images/allsteps.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b51b5d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "- What I have: a large set of videos for each step\n",
    "- What I need: images of hands for each step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc51525",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Reads a video file, extracts each frame, and saves the frames as JPEG images to a directory.\n",
    "It uses OpenCV to read the video, processes frames sequentially, and assigns filenames based on the frame number and the original video filename. \n",
    "The process continues until all frames are saved.\n",
    "\n",
    "```python\n",
    "def get_frames_from_video(directory, filename, step, output_frames_dir):\n",
    "    # Creating a VideoCapture object to read the video\n",
    "    cap = cv2.VideoCapture(os.path.join(directory, steps[step], filename))\n",
    "\n",
    "    is_success, image = cap.read()\n",
    "    frame_number = 0\n",
    "\n",
    "    while is_success:\n",
    "        out_filename = \"frame_{}_{}.jpg\".format(frame_number, os.path.splitext(filename)[0])\n",
    "        save_path_and_name = os.path.join(output_frames_dir, out_filename)\n",
    "        cv2.imwrite(save_path_and_name, image)\n",
    "        is_success, image = cap.read()\n",
    "        frame_number += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abebf44",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This code splits videos from each step into training and testing datasets based on a `test_ratio` of 30%, then saves the frames to the specific directory\n",
    "```python\n",
    "counter = 0\n",
    "test_ratio = 0.3\n",
    "\n",
    "for step in range(1,13):\n",
    "    counter = 0\n",
    "    for video in all_file_names_dict[step]:\n",
    "        if (video != \".DS_Store\"):\n",
    "\n",
    "            if ((len(all_file_names_dict) * (1-test_ratio) ) < counter):\n",
    "                #train data\n",
    "                get_frames_from_video(input_dir, video, step, output_frames_dir_train)\n",
    "            else:\n",
    "                #test data\n",
    "                get_frames_from_video(input_dir, video, step, output_frames_dir_test)\n",
    "            counter += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6e40d0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we have \n",
    "- output_frames_dir_train\n",
    "- output_frames_dir_test    \n",
    "Which contain the frames we need!!\n",
    "\n",
    "But there is no subdirectory order yet. We will need to make subdirectories for each step so tensorflow can distinguish between our classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb1473c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Moving test frames into their directories\n",
    "\n",
    "```python\n",
    "#moving all test photos step(1-9) into their respective directories\n",
    "for step in range(1, 10):\n",
    "    move_video_into_subdirectory_onedigit(output_frames_dir_test, \n",
    "                                          os.path.join(output_frames_dir_test,('step_' + str(step))), step)\n",
    "    #moving all test photos step(10-12) into their respective directories\n",
    "for step in range(10, 13):\n",
    "    move_video_into_subdirectory_twodigit(output_frames_dir_test, \n",
    "                                          os.path.join(output_frames_dir_test,('step_' + str(step))), step)\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9db2c7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Moving train photos into their directories\n",
    "```python\n",
    "#moving all train photos step(1-9) into their respective directories\n",
    "for step in range(1, 10):\n",
    "    move_video_into_subdirectory_onedigit(output_frames_dir_train, \n",
    "                                          os.path.join(output_frames_dir_train,('step_' + str(step))), step)\n",
    "    \n",
    "#moving all train photos step(10-12) into their respective directories\n",
    "for step in range(10, 13):\n",
    "    move_video_into_subdirectory_twodigit(output_frames_dir_train, \n",
    "                                          os.path.join(output_frames_dir_train,('step_' + str(step))), step)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e618f442",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating our datasets in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d434939",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Imports\n",
    "```python\n",
    "import matplotlib.pyplot as pl\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import PIL\n",
    "import pathlib\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, MaxPooling2D\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739b6270",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Now, we can create our training and validation data sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc026ac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This will create a shape of (32, 128, 128, 3)\n",
    "(16 = batch size, 128 = image width, 128 = image height, 3 = features (red green blue))\n",
    "```python\n",
    "# Parameters\n",
    "batch_size = 16\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "\n",
    "# Load training dataset\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    seed=123,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681d3ced",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Load testing dataset\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    seed=123,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cf1f45",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Lets test to see if our images have loaded correctly**\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44684a16",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/testcodeoutput.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf81aa7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Confirming the shape of our inputs\n",
    "\n",
    "```python\n",
    "for image_batch, labels_batch in train_ds:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break\n",
    "```\n",
    "> (16, 128, 128, 3)     \n",
    "> (16,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aa722e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# All of our images are loaded up, and we are ready to train our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b19c7b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A CNN is composed of many different layers. They are commonly used for image classification because images are so large!\n",
    "<img src=\"images/cnn_model.jpg\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6afd88",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/einstein_features.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121a9530",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is what an image is composed of. This is why we have 3 features in our shape (x, 128, 128, 3)\n",
    "<img src=\"images/dog_colors.png\" alt=\"drawing\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac906ae",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img src=\"images/feature_channels.png\" alt=\"drawing\" width=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae39e54",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**We are ready to start training. Lets create some data augmentation to introduce flexibility to our model.**\n",
    "```python\n",
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.RandomFlip(\"horizontal\",\n",
    "                      input_shape=(img_height,\n",
    "                                  img_width,\n",
    "                                  3)),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "  ]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafdd172",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Now we can create our model.**\n",
    "\n",
    "We will add every element to our model using model = Sequential([ ... ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8140b2cd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**data_augmentation**\n",
    "\n",
    "```python\n",
    "data_augmentation,\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36de31e4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**normalization**\n",
    "```python\n",
    "layers.Rescaling(1./255),\n",
    "# Large input values may lead to exploding gradients, making the training process erratic or causing the model to diverge.\n",
    "# Small input values can result in vanishing gradients, slowing down or completely halting learning. \n",
    "# Normalization keeps the inputs in a controlled range\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eac0fb0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**First Conv Block**\n",
    "```python\n",
    "layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "BatchNormalization(),\n",
    "layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7eeea3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Convolutions:\n",
    "Values of the kernel (filter) change as the model learned\n",
    "![gif](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D6iRfzDkz-sEzyjYoVZ73w.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa837330",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Relu\n",
    "\n",
    "Adds non-linearity to the model, which is necessary for making complex predicitions\n",
    "\n",
    "<img src=\"images/relu.jpg\" alt=\"relu\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2804df",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pooling Layer\n",
    "(2,2) pooling, in my model I used max pooling\n",
    "<img src=\"images/pooling.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f23dc19d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Batch Normalization\n",
    "- Normalizes the inputs at each stage\n",
    "<img src=\"images/batch_normalization.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bcccdb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Any layer that previously received x as the input, now receives BN(x).\n",
    "<img src=\"images/batch_transform.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2a679842",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       " \\[[3, 5, 8, 9, 11, 24]\\]\\[\\mu_B = \\frac{1}{m_B} \\sum_{i=1}^{m_B} x^{(i)} = \\frac{1}{6}(3 + 5 + 8 + 9 + 11 + 24) = 10\\]\\[\\sigma_B^2 = \\frac{1}{m_B} \\sum_{i=1}^{m_B} \\left(x^{(i)} - \\mu_B\\right)^2 = \\frac{1}{6}\\left((3 - 10)^2 + (5 - 10)^2 + \\dots + (24 - 10)^2\\right) = 46\\]\\[\\hat{x}^{(i)} = \\frac{x^{(i)} - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\\]\\[\\hat{x}^{(0)} = \\frac{3 - 10}{\\sqrt{46 + 0.00001}} = -1.03\\]\\[[-1.03, -0.74, -0.29, -0.15, 0.15, 2.06]\\]Mean = 0Std = 0.998\\[z^{(i)} = \\gamma \\otimes \\hat{x}^{(i)} + \\beta\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Latex(r\"\"\" \\[[3, 5, 8, 9, 11, 24]\\]\\[\\mu_B = \\frac{1}{m_B} \\sum_{i=1}^{m_B} x^{(i)} = \\frac{1}{6}(3 + 5 + 8 + 9 + 11 + 24) = 10\\]\\[\\sigma_B^2 = \\frac{1}{m_B} \\sum_{i=1}^{m_B} \\left(x^{(i)} - \\mu_B\\right)^2 = \\frac{1}{6}\\left((3 - 10)^2 + (5 - 10)^2 + \\dots + (24 - 10)^2\\right) = 46\\]\\[\\hat{x}^{(i)} = \\frac{x^{(i)} - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\\]\\[\\hat{x}^{(0)} = \\frac{3 - 10}{\\sqrt{46 + 0.00001}} = -1.03\\]\\[[-1.03, -0.74, -0.29, -0.15, 0.15, 2.06]\\]Mean = 0Std = 0.998\\[z^{(i)} = \\gamma \\otimes \\hat{x}^{(i)} + \\beta\\]\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d28c3ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Ioffe & Szegedy, 2015\n",
    "- <img src=\"images/batch_norm.png\" alt=\"drawing\" width=\"400\"/>\n",
    "```python \n",
    "layers.Rescaling(1./255) is like batchNormalization()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd29fee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Second Conv Block**\n",
    "```python\n",
    "layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "BatchNormalization(),\n",
    "layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83792555",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Third Conv Block**\n",
    "```python\n",
    "layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "BatchNormalization(),\n",
    "layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f74980d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Regularization**\n",
    "```python\n",
    "layers.Dropout(0.5),\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a21f280",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dropout\n",
    "\n",
    "Adds randomness to the model which is a form of regularization\n",
    "\n",
    "<img src=\"images/dropout.jpg\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022166ad",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Overfitting\n",
    "<img src=\"images/overfitting.jpg\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6d3b46",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Fully Connected Layers**\n",
    "```python\n",
    "layers.Flatten(),\n",
    "layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "BatchNormalization(),\n",
    "layers.Dropout(0.5),\n",
    "layers.Dense(num_classes, activation='softmax')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e04d70",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fully connected layers\n",
    "<img src=\"images/flattening.jpg\" alt=\"drawing\" width=\"200\"/>\n",
    "<img src=\"images/fullconnection.jpg\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0408615e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Review\n",
    "```model.summary()```\n",
    "<img src=\"images/model_summary.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e25e1ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define the optimizer\n",
    "\n",
    "```python\n",
    "#Adaptive Moment Estimation (Adam) - momentum and adaptive learning rates\n",
    "# keeps track of past gradients to adjust parameters (momentum)\n",
    "# changes learning rates based on how large the gradient is (adaptive learning rates)\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4403e499",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\[\\text{Loss} = - \\sum_{i=1}^{\\text{output size}} y_i \\cdot \\log \\hat{y}_i\\]\\[\\text{Multiclass Cross-Entropy}\\]\\[\\text{Loss}: \\text{ The overall loss function to minimize during training.}\\]\\[y_i: \\text{ The true label for the } i\\text{-th class (1 for correct class, 0 otherwise).}\\]\\[\\hat{y}_i: \\text{ The predicted probability for the } i\\text{-th class.}\\]\\[\\log \\hat{y}_i: \\text{ Logarithm of the predicted probability; penalizes incorrect predictions.}\\]\\[\\sum_{i=1}^{\\text{output size}}: \\text{ Summation over all classes in the output.}\\]\\[\\text{output size}: \\text{ Total number of classes in the problem.}\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Latex(r\"\"\"\\[\\text{Loss} = - \\sum_{i=1}^{\\text{output size}} y_i \\cdot \\log \\hat{y}_i\\]\\[\\text{Multiclass Cross-Entropy}\\]\\[\\text{Loss}: \\text{ The overall loss function to minimize during training.}\\]\\[y_i: \\text{ The true label for the } i\\text{-th class (1 for correct class, 0 otherwise).}\\]\\[\\hat{y}_i: \\text{ The predicted probability for the } i\\text{-th class.}\\]\\[\\log \\hat{y}_i: \\text{ Logarithm of the predicted probability; penalizes incorrect predictions.}\\]\\[\\sum_{i=1}^{\\text{output size}}: \\text{ Summation over all classes in the output.}\\]\\[\\text{output size}: \\text{ Total number of classes in the problem.}\\]\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0b2691",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Running the model\n",
    "\n",
    "```python\n",
    "epochs=5 \n",
    "\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d279ba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Results\n",
    "\n",
    "- 18000 photos per class\n",
    "- After 5 Epochs: accuracy: 0.8871 - loss: 1.2955 - val_accuracy: 0.9562 - val_loss: 1.0670"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9481ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/results_final2.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d8e1bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Great results with a seperate test dataset\n",
    "<img src=\"images/confusion_final2.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ccda03",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This came after lots of fine tuning, there were previous models that weren't very accurate. \n",
    "<img src=\"images/confusion_matrix_1.png\" alt=\"drawing\" width=\"200\"/>\n",
    "<img src=\"images/confusion_matrix_2.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d8764a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/confusion_matrix_3.png\" alt=\"drawing\" width=\"200\"/>\n",
    "<img src=\"images/confusion_final.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f39ac8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Live Demo\n",
    "https://youtu.be/moEpnU08rcM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f2d664",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Demo sudo code\n",
    "\n",
    "```python\n",
    "def initialize_camera()\n",
    "\n",
    "def take_photo(device_id, filename='photo.jpg', quality=0.8):\n",
    "    \n",
    "def preprocess_image(image_path, img_height=128, img_width=128, color_mode=\"rgb\"):\n",
    "    \n",
    "def classify_live(model, class_names, img_height=128, img_width=128, fps=1):\n",
    "    while True:\n",
    "        initialize_camera()\n",
    "        take_photo()\n",
    "        preprocess_image()\n",
    "        predictions = model.predict(processed_image)\n",
    "        dict.add(predictions)\n",
    "        print(dict, message)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9d755a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64be2a6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Live Prediction\n",
    "\n",
    "- I would like to make the prediction completely live with a current step label. It was difficult to do within Google Colab since I could only take so many frames per second with the javascript implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480a69db",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### More accuracy\n",
    "- My accuracy was around 88%, however, there still is room for improvement, especially for live use, many of the steps get mixed up with each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e49ab",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Processing power\n",
    "- 5 epochs took me ~30 minutes\n",
    "- This project was ran on A100 GPUs from Google Colab which I have through Colab Pro+\n",
    "- I had gotten access to TPUs from Google Cloud Services, but my dataset was taking too long to transfer over to the VM.\n",
    "- In the future, I want to try and use the TPUs or some other processor so I can do more than 5 epochs.\n",
    "<img src=\"images/tpus.png\" alt=\"drawing\" width=\"300\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c131fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example**:\n",
    "Step_1 and Step_4 get mixed up often:\n",
    "\n",
    "\n",
    "<img src=\"images/step_1_example.jpg\" alt=\"drawing\" width=\"200\"/> step 1\n",
    "<img src=\"images/step_4_example.jpg\" alt=\"drawing\" width=\"200\"/> step 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a9f7b7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And step 9 and step 10\n",
    "<img src=\"images/step_9_example.jpg\" alt=\"drawing\" width=\"200\"/> step 1\n",
    "<img src=\"images/step_10_example.jpg\" alt=\"drawing\" width=\"200\"/> step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd33934a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Try using a pre-built model by tensorflow\n",
    "- I saw that tensorflow had object detection models on github that could be used to make this better. I saw an implementation with sign language that worked really well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e41fd2b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The end"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
