{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21107267",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Final Project Presentation\n",
    "\n",
    "## Andrew Bahsoun\n",
    "\n",
    "## 11 December 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8693626a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Project Details\n",
    "Image Classification Model to determine what step someone is on when washing their hands.\n",
    "\n",
    "<img src=\"images/allsteps.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44a8af9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "- What I have: a large set of videos for each step\n",
    "- What I need: images of hands for each step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c652e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Reads a video file, extracts each frame, and saves the frames as JPEG images to a directory.\n",
    "It uses OpenCV to read the video, processes frames sequentially, and assigns filenames based on the frame number and the original video filename. \n",
    "The process continues until all frames are saved.\n",
    "\n",
    "```python\n",
    "def get_frames_from_video(directory, filename, step, output_frames_dir):\n",
    "    # Creating a VideoCapture object to read the video\n",
    "    cap = cv2.VideoCapture(os.path.join(directory, steps[step], filename))\n",
    "\n",
    "    is_success, image = cap.read()\n",
    "    frame_number = 0\n",
    "\n",
    "    while is_success:\n",
    "        out_filename = \"frame_{}_{}.jpg\".format(frame_number, os.path.splitext(filename)[0])\n",
    "        save_path_and_name = os.path.join(output_frames_dir, out_filename)\n",
    "        cv2.imwrite(save_path_and_name, image)\n",
    "        is_success, image = cap.read()\n",
    "        frame_number += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c0a133",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This code splits videos from each step into training and testing datasets based on a `test_ratio` of 30%, then saves the frames to the specific directory\n",
    "```python\n",
    "counter = 0\n",
    "test_ratio = 0.3\n",
    "\n",
    "for step in range(1,13):\n",
    "    counter = 0\n",
    "    for video in all_file_names_dict[step]:\n",
    "        if (video != \".DS_Store\"):\n",
    "\n",
    "            if ((len(all_file_names_dict) * (1-test_ratio) ) < counter):\n",
    "                #train data\n",
    "                get_frames_from_video(input_dir, video, step, output_frames_dir_train)\n",
    "            else:\n",
    "                #test data\n",
    "                get_frames_from_video(input_dir, video, step, output_frames_dir_test)\n",
    "            counter += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d6d383",
   "metadata": {},
   "source": [
    "Now we have \n",
    "- output_frames_dir_train\n",
    "- output_frames_dir_test    \n",
    "Which contain the frames we need!!\n",
    "\n",
    "But there is no subdirectory order yet. We will need to make subdirectories for each step so tensorflow can distinguish between our classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66684b0a",
   "metadata": {},
   "source": [
    "Moving test frames into their directories\n",
    "\n",
    "```python\n",
    "#moving all test photos step(1-9) into their respective directories\n",
    "for step in range(1, 10):\n",
    "    move_video_into_subdirectory_onedigit(output_frames_dir_test, \n",
    "                                          os.path.join(output_frames_dir_test,('step_' + str(step))), step)\n",
    "    #moving all test photos step(10-12) into their respective directories\n",
    "for step in range(10, 13):\n",
    "    move_video_into_subdirectory_twodigit(output_frames_dir_test, \n",
    "                                          os.path.join(output_frames_dir_test,('step_' + str(step))), step)\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1f989d",
   "metadata": {},
   "source": [
    "Moving train photos into their directories\n",
    "```python\n",
    "#moving all train photos step(1-9) into their respective directories\n",
    "for step in range(1, 10):\n",
    "    move_video_into_subdirectory_onedigit(output_frames_dir_train, \n",
    "                                          os.path.join(output_frames_dir_train,('step_' + str(step))), step)\n",
    "    \n",
    "#moving all train photos step(10-12) into their respective directories\n",
    "for step in range(10, 13):\n",
    "    move_video_into_subdirectory_twodigit(output_frames_dir_train, \n",
    "                                          os.path.join(output_frames_dir_train,('step_' + str(step))), step)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a10c04a",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa49a67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
