{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aad0f582",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Final Project Presentation\n",
    "\n",
    "## Andrew Bahsoun\n",
    "\n",
    "## 11 December 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86b4f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ed2436",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Project Details\n",
    "Image Classification Model to determine what step someone is on when washing their hands.\n",
    "\n",
    "<img src=\"images/allsteps.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a87db4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "- What I have: a large set of videos for each step\n",
    "- What I need: images of hands for each step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87728944",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Reads a video file, extracts each frame, and saves the frames as JPEG images to a directory.\n",
    "It uses OpenCV to read the video, processes frames sequentially, and assigns filenames based on the frame number and the original video filename. \n",
    "The process continues until all frames are saved.\n",
    "\n",
    "```python\n",
    "def get_frames_from_video(directory, filename, step, output_frames_dir):\n",
    "    # Creating a VideoCapture object to read the video\n",
    "    cap = cv2.VideoCapture(os.path.join(directory, steps[step], filename))\n",
    "\n",
    "    is_success, image = cap.read()\n",
    "    frame_number = 0\n",
    "\n",
    "    while is_success:\n",
    "        out_filename = \"frame_{}_{}.jpg\".format(frame_number, os.path.splitext(filename)[0])\n",
    "        save_path_and_name = os.path.join(output_frames_dir, out_filename)\n",
    "        cv2.imwrite(save_path_and_name, image)\n",
    "        is_success, image = cap.read()\n",
    "        frame_number += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a934b395",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This code splits videos from each step into training and testing datasets based on a `test_ratio` of 30%, then saves the frames to the specific directory\n",
    "```python\n",
    "counter = 0\n",
    "test_ratio = 0.3\n",
    "\n",
    "for step in range(1,13):\n",
    "    counter = 0\n",
    "    for video in all_file_names_dict[step]:\n",
    "        if (video != \".DS_Store\"):\n",
    "\n",
    "            if ((len(all_file_names_dict) * (1-test_ratio) ) < counter):\n",
    "                #train data\n",
    "                get_frames_from_video(input_dir, video, step, output_frames_dir_train)\n",
    "            else:\n",
    "                #test data\n",
    "                get_frames_from_video(input_dir, video, step, output_frames_dir_test)\n",
    "            counter += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a4b64a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we have \n",
    "- output_frames_dir_train\n",
    "- output_frames_dir_test    \n",
    "Which contain the frames we need!!\n",
    "\n",
    "But there is no subdirectory order yet. We will need to make subdirectories for each step so tensorflow can distinguish between our classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052e2e97",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Moving test frames into their directories\n",
    "\n",
    "```python\n",
    "#moving all test photos step(1-9) into their respective directories\n",
    "for step in range(1, 10):\n",
    "    move_video_into_subdirectory_onedigit(output_frames_dir_test, \n",
    "                                          os.path.join(output_frames_dir_test,('step_' + str(step))), step)\n",
    "    #moving all test photos step(10-12) into their respective directories\n",
    "for step in range(10, 13):\n",
    "    move_video_into_subdirectory_twodigit(output_frames_dir_test, \n",
    "                                          os.path.join(output_frames_dir_test,('step_' + str(step))), step)\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f496ccc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Moving train photos into their directories\n",
    "```python\n",
    "#moving all train photos step(1-9) into their respective directories\n",
    "for step in range(1, 10):\n",
    "    move_video_into_subdirectory_onedigit(output_frames_dir_train, \n",
    "                                          os.path.join(output_frames_dir_train,('step_' + str(step))), step)\n",
    "    \n",
    "#moving all train photos step(10-12) into their respective directories\n",
    "for step in range(10, 13):\n",
    "    move_video_into_subdirectory_twodigit(output_frames_dir_train, \n",
    "                                          os.path.join(output_frames_dir_train,('step_' + str(step))), step)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92ad76a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating our datasets in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f9efac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Imports\n",
    "```python\n",
    "import matplotlib.pyplot as pl\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import PIL\n",
    "import pathlib\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, MaxPooling2D\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf2a5f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Now, we can create our training and validation data sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d501cf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This will create a shape of (32, 128, 128, 3)\n",
    "(16 = batch size, 128 = image width, 128 = image height, 3 = features (red green blue))\n",
    "```python\n",
    "# Parameters\n",
    "batch_size = 16\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "\n",
    "# Load training dataset\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    seed=123,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e363b7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Load testing dataset\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    seed=123,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeba3d9a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Lets test to see if our images have loaded correctly**\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6663802a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/testcodeoutput.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e1760d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# All of our images are loaded up, and we are ready to train our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df043e4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A CNN is composed of many different layers. They are commonly used for image classification because images are so large!\n",
    "<img src=\"images/cnn_model.jpg\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a73fd10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is what an image is composed of. This is why we have 3 features in our shape (x, 128, 128, 3)\n",
    "<img src=\"images/dog_colors.png\" alt=\"drawing\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23effee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img src=\"images/feature_channels.png\" alt=\"drawing\" width=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb499e8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**We are ready to start training. Lets create some data augmentation to introduce flexibility to our model.**\n",
    "```python\n",
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.RandomFlip(\"horizontal\",\n",
    "                      input_shape=(img_height,\n",
    "                                  img_width,\n",
    "                                  3)),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "  ]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d0527",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Now we can create our model.**\n",
    "\n",
    "We will add every element to our model using model = Sequential([ ... ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0084f1aa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**data_augmentation**\n",
    "\n",
    "```python\n",
    "data_augmentation,\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d12e2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**normalization**\n",
    "```python\n",
    "layers.Rescaling(1./255),\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c97fcb6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**First Conv Block**\n",
    "```python\n",
    "layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "BatchNormalization(),\n",
    "layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88365b5d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Convolutions:\n",
    "![gif](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D6iRfzDkz-sEzyjYoVZ73w.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02698e94",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Relu\n",
    "\n",
    "Adds non-linearity to the model\n",
    "\n",
    "<img src=\"images/relu.jpg\" alt=\"relu\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3df10377",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\[\\text{Loss} = - \\sum_{i=1}^{\\text{output size}} y_i \\cdot \\log \\hat{y}_i\\]\\[\\text{Multiclass Cross-Entropy}\\]\\[\\text{Loss}: \\text{ The overall loss function to minimize during training.}\\]\\[y_i: \\text{ The true label for the } i\\text{-th class (1 for correct class, 0 otherwise).}\\]\\[\\hat{y}_i: \\text{ The predicted probability for the } i\\text{-th class.}\\]\\[\\log \\hat{y}_i: \\text{ Logarithm of the predicted probability; penalizes incorrect predictions.}\\]\\[\\sum_{i=1}^{\\text{output size}}: \\text{ Summation over all classes in the output.}\\]\\[\\text{output size}: \\text{ Total number of classes in the problem.}\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Latex(r\"\"\"\\[\\text{Loss} = - \\sum_{i=1}^{\\text{output size}} y_i \\cdot \\log \\hat{y}_i\\]\\[\\text{Multiclass Cross-Entropy}\\]\\[\\text{Loss}: \\text{ The overall loss function to minimize during training.}\\]\\[y_i: \\text{ The true label for the } i\\text{-th class (1 for correct class, 0 otherwise).}\\]\\[\\hat{y}_i: \\text{ The predicted probability for the } i\\text{-th class.}\\]\\[\\log \\hat{y}_i: \\text{ Logarithm of the predicted probability; penalizes incorrect predictions.}\\]\\[\\sum_{i=1}^{\\text{output size}}: \\text{ Summation over all classes in the output.}\\]\\[\\text{output size}: \\text{ Total number of classes in the problem.}\\]\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e554e76",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pooling Layer\n",
    "(2,2) pooling, in my model I used max pooling\n",
    "<img src=\"images/pooling.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a925d7d0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Batch Normalization\n",
    "- Normalizes the inputs at each stage\n",
    "<img src=\"images/batch_normalization.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "330333da",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       " \\[[3, 5, 8, 9, 11, 24]\\]\\[\\mu_B = \\frac{1}{m_B} \\sum_{i=1}^{m_B} x^{(i)} = \\frac{1}{6}(3 + 5 + 8 + 9 + 11 + 24) = 10\\]\\[\\sigma_B^2 = \\frac{1}{m_B} \\sum_{i=1}^{m_B} \\left(x^{(i)} - \\mu_B\\right)^2 = \\frac{1}{6}\\left((3 - 10)^2 + (5 - 10)^2 + \\dots + (24 - 10)^2\\right) = 46\\]\\[\\hat{x}^{(i)} = \\frac{x^{(i)} - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\\]\\[\\hat{x}^{(0)} = \\frac{3 - 10}{\\sqrt{46 + 0.00001}} = -1.03\\]\\[[-1.03, -0.74, -0.29, -0.15, 0.15, 2.06]\\]Mean = 0Std = 0.998\\[z^{(i)} = \\gamma \\otimes \\hat{x}^{(i)} + \\beta\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Latex(r\"\"\" \\[[3, 5, 8, 9, 11, 24]\\]\\[\\mu_B = \\frac{1}{m_B} \\sum_{i=1}^{m_B} x^{(i)} = \\frac{1}{6}(3 + 5 + 8 + 9 + 11 + 24) = 10\\]\\[\\sigma_B^2 = \\frac{1}{m_B} \\sum_{i=1}^{m_B} \\left(x^{(i)} - \\mu_B\\right)^2 = \\frac{1}{6}\\left((3 - 10)^2 + (5 - 10)^2 + \\dots + (24 - 10)^2\\right) = 46\\]\\[\\hat{x}^{(i)} = \\frac{x^{(i)} - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\\]\\[\\hat{x}^{(0)} = \\frac{3 - 10}{\\sqrt{46 + 0.00001}} = -1.03\\]\\[[-1.03, -0.74, -0.29, -0.15, 0.15, 2.06]\\]Mean = 0Std = 0.998\\[z^{(i)} = \\gamma \\otimes \\hat{x}^{(i)} + \\beta\\]\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e90eb4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Second Conv Block**\n",
    "```python\n",
    "layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "BatchNormalization(),\n",
    "layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3f20da",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Third Conv Block**\n",
    "```python\n",
    "layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "BatchNormalization(),\n",
    "layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f2c96b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Regularization**\n",
    "```python\n",
    "layers.Dropout(0.5),\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4402f0e7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Fully Connected Layers**\n",
    "```python\n",
    "layers.Flatten(),\n",
    "layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "BatchNormalization(),\n",
    "layers.Dropout(0.5),\n",
    "layers.Dense(num_classes, activation='softmax')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce63526f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
